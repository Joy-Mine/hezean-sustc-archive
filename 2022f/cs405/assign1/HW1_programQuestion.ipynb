{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_data(func, sample_size, std):\n",
    "    x = np.linspace(0, 1, sample_size)\n",
    "    t = func(x) + np.random.normal(scale=std, size=x.shape)\n",
    "    return x, t\n",
    "\n",
    "def func(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "x_train, y_train = create_toy_data(func, 10, 0.25)\n",
    "x_test = np.linspace(0, 1, 100)\n",
    "y_test = func(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Plot the graph with given code, the result should be same as this.\n",
    "![](originalData.png)\n",
    "`x_train` and `y_train` are the datas you need to create, `sample_size` is 10 and `std` is 0.25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, y_test, label='sin(2πx)', color='green')\n",
    "plt.scatter(x_train, y_train, label='training data', c='none', edgecolors='blue')\n",
    "\n",
    "# plt.xticks(np.arange(0, 1.1, 0.2))\n",
    "# plt.yticks(np.arange(-1, 1.3, 0.5))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) On the basis of the results, you should try $0^{th}$ order polynomial, $1^{st}$ order polynomial, $3^{rd}$ order polynomial and some other order polynomial, show the results include fitting and over-fitting.\n",
    "![](fitting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "\n",
    "class PolynomialFeature(object):\n",
    "    \"\"\"\n",
    "    polynomial features\n",
    "\n",
    "    transforms input array with polynomial features\n",
    "\n",
    "    Example\n",
    "    =======\n",
    "    x =\n",
    "    [[a, b],\n",
    "    [c, d]]\n",
    "\n",
    "    y = PolynomialFeatures(degree=2).transform(x)\n",
    "    y =\n",
    "    [[1, a, b, a^2, a * b, b^2],\n",
    "    [1, c, d, c^2, c * d, d^2]]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degree=2):\n",
    "        \"\"\"\n",
    "        construct polynomial features\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        degree : int\n",
    "            degree of polynomial\n",
    "        \"\"\"\n",
    "        assert isinstance(degree, int)\n",
    "        self.degree = degree\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"\n",
    "        transforms input array with polynomial features\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (sample_size, n) ndarray\n",
    "            input array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : (sample_size, 1 + nC1 + ... + nCd) ndarray\n",
    "            polynomial features\n",
    "        \"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        x_t = x.transpose()\n",
    "        features = [np.ones(len(x))]\n",
    "        for degree in range(1, self.degree + 1):\n",
    "            for items in itertools.combinations_with_replacement(x_t, degree):\n",
    "                features.append(functools.reduce(lambda x, y: x * y, items))\n",
    "        return np.asarray(features).transpose()\n",
    "    \n",
    "class Regression(object):\n",
    "    \"\"\"\n",
    "    Base class for regressors\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class LinearRegression(Regression):\n",
    "    \"\"\"\n",
    "    Linear regression model\n",
    "    y = X @ w\n",
    "    t ~ N(t|X @ w, var)\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X:np.ndarray, t:np.ndarray):\n",
    "        \"\"\"\n",
    "        perform least squares fitting\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, D) np.ndarray\n",
    "            training independent variable\n",
    "        t : (N,) np.ndarray\n",
    "            training dependent variable\n",
    "        \"\"\"\n",
    "        self.w = np.linalg.pinv(X) @ t\n",
    "        self.var = np.mean(np.square(X @ self.w - t))\n",
    "\n",
    "    def predict(self, X:np.ndarray, return_std:bool=False):\n",
    "        \"\"\"\n",
    "        make prediction given input\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, D) np.ndarray\n",
    "            samples to predict their output\n",
    "        return_std : bool, optional\n",
    "            returns standard deviation of each predition if True\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (N,) np.ndarray\n",
    "            prediction of each sample\n",
    "        y_std : (N,) np.ndarray\n",
    "            standard deviation of each predition\n",
    "        \"\"\"\n",
    "        y = X @ self.w\n",
    "        if return_std:\n",
    "            y_std = np.sqrt(self.var) + np.zeros_like(y)\n",
    "            return y, y_std\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def plot_polynomial(x_train: np.array, y_train: np.array,\n",
    "          x_test: np.array, y_test: np.array,\n",
    "          degree: int, index: Tuple[int, int, int], weights: map=None):\n",
    "    pf = PolynomialFeature(degree)\n",
    "    feat_train, feat_test = pf.transform(x_train), pf.transform(x_test)\n",
    "    model = LinearRegression()\n",
    "    model.fit(feat_train, y_train)\n",
    "    y_pred = model.predict(feat_test)\n",
    "    if weights is not None:\n",
    "        weights[degree] = model.w[-1]\n",
    "    plt.subplot(*index)\n",
    "    plt.title(f'{degree} order polynomial, {len(x_train)} samples')\n",
    "    plt.plot(x_test, y_test, label='sin(2πx)', color='green')\n",
    "    plt.scatter(x_train, y_train, label='training data', c='none', edgecolors='blue')\n",
    "    plt.plot(x_test, y_pred, label=f'{degree}-order pred', color='red')\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_polynomial(x_train, y_train, x_test, y_test, 0, (2, 2, 1))\n",
    "plot_polynomial(x_train, y_train, x_test, y_test, 1, (2, 2, 2))\n",
    "plot_polynomial(x_train, y_train, x_test, y_test, 3, (2, 2, 3))\n",
    "plot_polynomial(x_train, y_train, x_test, y_test, 10, (2, 2, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Plot the graph of the root-mean-square error.\n",
    "![](rmse.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def rmse(a, b):\n",
    "    mse = np.mean(np.square(a - b))\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "def train_test_rmse(x_train, y_train, x_test, y_test, degree) -> Tuple[float, float]:\n",
    "    pf = PolynomialFeature(degree)\n",
    "    feat_train, feat_test = pf.transform(x_train), pf.transform(x_test)\n",
    "    model = LinearRegression()\n",
    "    model.fit(feat_train, y_train)\n",
    "    y_train_pred = model.predict(feat_train)\n",
    "    y_test_pred = model.predict(feat_test)\n",
    "    return rmse(y_train, y_train_pred), rmse(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for degree in range(11):\n",
    "    rmse_train, rmse_test = train_test_rmse(x_train, y_train, x_test, y_test, degree)\n",
    "    training_errors.append(rmse_train)\n",
    "    test_errors.append(rmse_test)\n",
    "    \n",
    "plt.scatter(range(11), training_errors, label='train')\n",
    "plt.plot(range(11), training_errors)\n",
    "\n",
    "plt.scatter(range(11), test_errors, label='test')\n",
    "plt.plot(range(11), test_errors)\n",
    "\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Plot the graph of the predictive distribution resulting from a Bayesian treatment of polynomial curve fitting using an M=9 polynomial, with the fixed parameters $\\alpha=5\\times 10^{-3}$ and $\\beta=11.1$(corresponding to the known noise variance).\n",
    "![](bayesianRegression.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRegression(Regression):\n",
    "    \"\"\"\n",
    "    Bayesian regression model\n",
    "\n",
    "    w ~ N(w|0, alpha^(-1)I)\n",
    "    y = X @ w\n",
    "    t ~ N(t|X @ w, beta^(-1))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha:float=1., beta:float=1.):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.w_mean = None\n",
    "        self.w_precision = None\n",
    "\n",
    "    def _is_prior_defined(self) -> bool:\n",
    "        return self.w_mean is not None and self.w_precision is not None\n",
    "\n",
    "    def _get_prior(self, ndim:int) -> tuple:\n",
    "        if self._is_prior_defined():\n",
    "            return self.w_mean, self.w_precision\n",
    "        else:\n",
    "            return np.zeros(ndim), self.alpha * np.eye(ndim)\n",
    "\n",
    "    def fit(self, X:np.ndarray, t:np.ndarray):\n",
    "        \"\"\"\n",
    "        bayesian update of parameters given training dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, n_features) np.ndarray\n",
    "            training data independent variable\n",
    "        t : (N,) np.ndarray\n",
    "            training data dependent variable\n",
    "        \"\"\"\n",
    "\n",
    "        mean_prev, precision_prev = self._get_prior(np.size(X, 1))\n",
    "\n",
    "        w_precision = precision_prev + self.beta * X.T @ X\n",
    "        w_mean = np.linalg.solve(\n",
    "            w_precision,\n",
    "            precision_prev @ mean_prev + self.beta * X.T @ t\n",
    "        )\n",
    "        self.w_mean = w_mean\n",
    "        self.w_precision = w_precision\n",
    "        self.w_cov = np.linalg.inv(self.w_precision)\n",
    "\n",
    "    def predict(self, X:np.ndarray, return_std:bool=False, sample_size:int=None):\n",
    "        \"\"\"\n",
    "        return mean (and standard deviation) of predictive distribution\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (N, n_features) np.ndarray\n",
    "            independent variable\n",
    "        return_std : bool, optional\n",
    "            flag to return standard deviation (the default is False)\n",
    "        sample_size : int, optional\n",
    "            number of samples to draw from the predictive distribution\n",
    "            (the default is None, no sampling from the distribution)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (N,) np.ndarray\n",
    "            mean of the predictive distribution\n",
    "        y_std : (N,) np.ndarray\n",
    "            standard deviation of the predictive distribution\n",
    "        y_sample : (N, sample_size) np.ndarray\n",
    "            samples from the predictive distribution\n",
    "        \"\"\"\n",
    "\n",
    "        if sample_size is not None:\n",
    "            w_sample = np.random.multivariate_normal(\n",
    "                self.w_mean, self.w_cov, size=sample_size\n",
    "            )\n",
    "            y_sample = X @ w_sample.T\n",
    "            return y_sample\n",
    "        y = X @ self.w_mean\n",
    "        if return_std:\n",
    "            y_var = 1 / self.beta + np.sum(X @ self.w_cov * X, axis=1)\n",
    "            y_std = np.sqrt(y_var)\n",
    "            return y, y_std\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeature(9)\n",
    "feat_train, feat_test = pf.transform(x_train), pf.transform(x_test)\n",
    "\n",
    "bayes = BayesianRegression(alpha=5*10**-3, beta=11.1)\n",
    "bayes.fit(feat_train, y_train)\n",
    "y_pred, y_std = bayes.predict(feat_test, return_std=True)\n",
    "\n",
    "plt.plot(x_test, y_test, label='sin(2πx)')\n",
    "plt.plot(x_test, y_pred, label='bayes')\n",
    "plt.scatter(x_train, y_train, label='training data', c='none', edgecolors='gray')\n",
    "plt.fill_between(x_test, y_pred - y_std, y_pred + y_std, alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Change the $sample\\_size$ to 2, 3 or 10 times than before, explain the change of $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(func, sample_size, w):\n",
    "    x_train, y_train = create_toy_data(func, sample_size, 0.25)\n",
    "    x_test = np.linspace(0, 1, 100)\n",
    "    y_test = func(x_test)\n",
    "\n",
    "    plt.figure(figsize=(16, 3))\n",
    "    plot_polynomial(x_train, y_train, x_test, y_test, 0, (1, 4, 1), w)\n",
    "    plot_polynomial(x_train, y_train, x_test, y_test, 1, (1, 4, 2), w)\n",
    "    plot_polynomial(x_train, y_train, x_test, y_test, 3, (1, 4, 3), w)\n",
    "    plot_polynomial(x_train, y_train, x_test, y_test, 10, (1, 4, 4), w)\n",
    "    plt.show()\n",
    "\n",
    "w20, w30, w100 = {}, {}, {}\n",
    "plot(func, 20, w20)\n",
    "plot(func, 30, w30)\n",
    "plot(func, 100, w100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "\n",
    "\n",
    "m0 = {20: w20[0], 30: w30[0], 100: w100[0]}\n",
    "m1 = {20: w20[1], 30: w30[1], 100: w100[1]}\n",
    "m3 = {20: w20[3], 30: w30[3], 100: w100[3]}\n",
    "m10 = {20: w20[10], 30: w30[10], 100: w100[10]}\n",
    "\n",
    "display(m0.values(), m1.values(), m3.values(), m10.values())\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(m0.keys(), m0.values(), marker='p', label='M = 0')\n",
    "plt.plot(m1.keys(), m1.values(), marker='p', label='M = 1')\n",
    "plt.plot(m3.keys(), m3.values(), marker='p', label='M = 3')\n",
    "plt.plot(m10.keys(), m10.values(), marker='p', label='M = 10')\n",
    "\n",
    "plt.xlabel('sample size')\n",
    "plt.ylabel('max norm of w')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above regressions, we can easily find out that:\n",
    "1. with $M$ increasing, it's more possible that the overfitting will be more serious\n",
    "2. with sample size increasing, it can depress the overfitting, yet can't avoid it\n",
    "3. larger sample size combined with appropriate degree can guide the trained model closer to actual distribution of the data\n",
    "4. with fixed sample data, the accuracy of trained model saturated with certain $M$ and larger degree will not bring benifit to accuracy but only bring overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sci39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d41f7ce290d37fe5b4ab4e5bf6b63bd16e05157bfb17b66f4f27316420edd9a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
