{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 Assignment\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (20 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12\n",
       "0   1   1   1   1   0   1   1   0   1   1   1   1   0\n",
       "1   0   1   1   1   0   1   1   0   1   1   1   1   0\n",
       "2   1   1   0   1   0   1   1   0   1   1   1   1   0\n",
       "3   1   1   1   1   0   1   1   0   1   1   1   0   0\n",
       "4   1   1   1   1   0   1   1   0   1   0   1   1   0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/digit01.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 12)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "y = df[12]\n",
    "X = df.drop(12, axis=1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Splitting dataset into 80% Training and 20% Testing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51, 12), (13, 12), (51,), (13,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import from_numpy\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.],\n",
       "        [1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.],\n",
       "        [0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.],\n",
       "        [1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.],\n",
       "        [1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0.],\n",
       "        [1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.],\n",
       "        [1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
       "        [0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0.],\n",
       "        [1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.],\n",
       "        [0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.],\n",
       "        [1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
       "        [0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.],\n",
       "        [0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a LogisticRegression subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "\n",
    "from torch import nn\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(12, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression().to(device)\n",
    "logistic_reg.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss(reduction='sum')\n",
    "# loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(logistic_reg.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]/Users/hezean/opt/anaconda3/envs/sci39/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([51])) that is different to the input size (torch.Size([51, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6557, 0.6336],\n",
      "        [0.5659, 0.3576],\n",
      "        [0.7220, 0.4804],\n",
      "        [0.5168, 0.4850],\n",
      "        [0.5619, 0.5820],\n",
      "        [0.5670, 0.5438],\n",
      "        [0.4628, 0.5330],\n",
      "        [0.6304, 0.4365],\n",
      "        [0.5562, 0.6234],\n",
      "        [0.5379, 0.6233],\n",
      "        [0.6042, 0.4487],\n",
      "        [0.5595, 0.4000],\n",
      "        [0.5818, 0.5919],\n",
      "        [0.4956, 0.3607],\n",
      "        [0.7504, 0.5217],\n",
      "        [0.6265, 0.6555],\n",
      "        [0.5198, 0.5839],\n",
      "        [0.5147, 0.4552],\n",
      "        [0.6170, 0.6312],\n",
      "        [0.6978, 0.5620],\n",
      "        [0.5636, 0.4461],\n",
      "        [0.4877, 0.4629],\n",
      "        [0.5571, 0.4543],\n",
      "        [0.6192, 0.4958],\n",
      "        [0.6401, 0.3597],\n",
      "        [0.5188, 0.4039],\n",
      "        [0.5615, 0.4169],\n",
      "        [0.6576, 0.6608],\n",
      "        [0.6522, 0.4619],\n",
      "        [0.5581, 0.6329],\n",
      "        [0.5752, 0.4544],\n",
      "        [0.5518, 0.5758],\n",
      "        [0.5923, 0.3672],\n",
      "        [0.6271, 0.4064],\n",
      "        [0.5014, 0.5838],\n",
      "        [0.5822, 0.4445],\n",
      "        [0.7135, 0.5604],\n",
      "        [0.6160, 0.7095],\n",
      "        [0.5273, 0.4057],\n",
      "        [0.5939, 0.6704],\n",
      "        [0.5256, 0.5413],\n",
      "        [0.6731, 0.3986],\n",
      "        [0.6073, 0.6253],\n",
      "        [0.6052, 0.3631],\n",
      "        [0.5252, 0.3773],\n",
      "        [0.5581, 0.6329],\n",
      "        [0.4946, 0.5836],\n",
      "        [0.6787, 0.4570],\n",
      "        [0.6290, 0.4163],\n",
      "        [0.4946, 0.4461],\n",
      "        [0.4651, 0.5331]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (51) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(y_pred)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(y_train)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(y_pred, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train\u001b[39m.\u001b[39mset_description(\u001b[39m'\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sci39/lib/python3.9/site-packages/torch/nn/modules/module.py:1363\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1361\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1362\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1363\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1364\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sci39/lib/python3.9/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sci39/lib/python3.9/site-packages/torch/nn/functional.py:3291\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3289\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3291\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3292\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sci39/lib/python3.9/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (51) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train = tqdm(range(2000))\n",
    "for t in train:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = logistic_reg(X_train)\n",
    "    print(y_pred)\n",
    "    print(y_train)\n",
    "    loss = loss_func(y_pred, y_train)\n",
    "    train.set_description('loss: %.3f' % loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predicted_cls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#classification report\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hezean/Developer/sustech-semester/cs405/lab7-nn/Lab7.Neural-network_12012929.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test, y_predicted_cls))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_predicted_cls' is not defined"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a MLP subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    1]. Loss: 2.309. \n",
      "[1,  101]. Loss: 2.294. \n",
      "[1,  201]. Loss: 2.274. \n",
      "[1,  301]. Loss: 2.258. \n",
      "[1,  401]. Loss: 2.235. \n",
      "[1,  501]. Loss: 2.204. \n",
      "[2,    1]. Loss: 2.190. \n",
      "[2,  101]. Loss: 2.191. \n",
      "[2,  201]. Loss: 2.183. \n",
      "[2,  301]. Loss: 2.108. \n",
      "[2,  401]. Loss: 2.100. \n",
      "[2,  501]. Loss: 2.072. \n",
      "[3,    1]. Loss: 2.089. \n",
      "[3,  101]. Loss: 2.069. \n",
      "[3,  201]. Loss: 2.044. \n",
      "[3,  301]. Loss: 2.019. \n",
      "[3,  401]. Loss: 1.997. \n",
      "[3,  501]. Loss: 1.943. \n",
      "[4,    1]. Loss: 1.942. \n",
      "[4,  101]. Loss: 1.904. \n",
      "[4,  201]. Loss: 1.873. \n",
      "[4,  301]. Loss: 1.828. \n",
      "[4,  401]. Loss: 1.782. \n",
      "[4,  501]. Loss: 1.818. \n",
      "[5,    1]. Loss: 1.762. \n",
      "[5,  101]. Loss: 1.733. \n",
      "[5,  201]. Loss: 1.644. \n",
      "[5,  301]. Loss: 1.634. \n",
      "[5,  401]. Loss: 1.577. \n",
      "[5,  501]. Loss: 1.544. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 75.412\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  Questions (10 points )\n",
    "1.What's the difference between logistic regression and Perceptron?\n",
    "\n",
    "2.Advantages and disadvantages of neural networks?\n",
    "\n",
    "3.What is the role of Activation Function in Neural networks?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sci39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d41f7ce290d37fe5b4ab4e5bf6b63bd16e05157bfb17b66f4f27316420edd9a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
