{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Southern University of Science and Technology-Department of Computer Science and Engineering\n",
    "\n",
    "Course: Machine Learning(CS 405)-Professor: Qi Hao\n",
    "\n",
    "## Homework #2\n",
    "#### Due date: October, 7th, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import libraries that you might require.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the KNN algorithm for the breast cancer dataset. Refer to the pdf and the following functions for the instructions. Complete all the functions as indicated below. The four functions would be autograded as mentioned in the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 1: Classification\n",
    "\n",
    "Please implement KNN for K: 3, 5, and 7 with the following norms:\n",
    "L1\n",
    "L2\n",
    "L-inf\n",
    "\"\"\"\n",
    "\n",
    "from itertools import product\n",
    "from typing import Counter\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast = load_breast_cancer()\n",
    "\n",
    "X = breast['data']\n",
    "y = breast['target']\n",
    "\n",
    "np.random.seed(100)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "X_train, y_train = X[:400], y[:400]\n",
    "X_val, y_val = X[400:500], y[400:500]\n",
    "X_test, y_test = X[500:], y[500:]\n",
    "\n",
    "\n",
    "def distanceFunc(metric_type, vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the distance between two d-dimension vectors. \n",
    "    \n",
    "    Please DO NOT use Numpy's norm function when implementing this function. \n",
    "    \n",
    "    Args:\n",
    "        metric_type (str): Metric: L1, L2, or L-inf\n",
    "        vec1 ((d,) np.ndarray): d-dim vector\n",
    "        vec2 ((d,)) np.ndarray): d-dim vector\n",
    "    \n",
    "    Returns:\n",
    "        distance (float): distance between the two vectors\n",
    "    \"\"\"\n",
    "\n",
    "    if metric_type == \"L1\":\n",
    "        distance = np.sum(np.abs(vec1 - vec2))\n",
    "\n",
    "    if metric_type == \"L2\":\n",
    "        distance = np.sqrt(np.sum(np.square(vec1 - vec2)))\n",
    "\n",
    "    if metric_type == \"L-inf\":\n",
    "        distance = np.max(np.abs(vec1 - vec2))\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def computeDistancesNeighbors(K, metric_type, X_train, y_train, sample):\n",
    "    \"\"\"\n",
    "    Compute the distances between every datapoint in the train_data and the \n",
    "    given sample. Then, find the k-nearest neighbors.\n",
    "    \n",
    "    Return a numpy array of the label of the k-nearest neighbors.\n",
    "    \n",
    "    Args:\n",
    "        K (int): K-value\n",
    "        metric_type (str): metric type\n",
    "        X_train ((n,p) np.ndarray): Training data with n samples and p features\n",
    "        y_train : Training labels\n",
    "        sample ((p,) np.ndarray): Single sample whose distance is to computed with every entry in the dataset\n",
    "        \n",
    "    Returns:\n",
    "        neighbors (list): K-nearest neighbors' labels\n",
    "    \"\"\"\n",
    "\n",
    "    dist = [distanceFunc(metric_type, sample, X_train[i]) for i in range(len(X_train))]\n",
    "    return [y_train[idx] for idx in np.argsort(dist, kind='stable')[:K]]\n",
    "\n",
    "\n",
    "def Majority(neighbors):\n",
    "    \"\"\"\n",
    "    Performs majority voting and returns the predicted value for the test sample.\n",
    "    \n",
    "    Since we're performing binary classification the possible values are [0,1].\n",
    "    \n",
    "    Args:\n",
    "        neighbors (list): K-nearest neighbors' labels\n",
    "        \n",
    "    Returns:\n",
    "        predicted_value (int): predicted label for the given sample\n",
    "    \"\"\"\n",
    "    cnt = Counter(neighbors)\n",
    "    return cnt.most_common(1)[0][0]  # fixme\n",
    "\n",
    "\n",
    "def KNN(K, metric_type, X_train, y_train, X_val):\n",
    "    \"\"\"\n",
    "    Returns the predicted values for the entire validation or test set.\n",
    "    \n",
    "    Please DO NOT use Scikit's KNN model when implementing this function. \n",
    "\n",
    "    Args:\n",
    "        K (int): K-value\n",
    "        metric_type (str): metric type\n",
    "        X_train ((n,p) np.ndarray): Training data with n samples and p features\n",
    "        y_train : Training labels\n",
    "        X_val ((n, p) np.ndarray): Validation or test data\n",
    "        \n",
    "    Returns:\n",
    "        predicted_values (list): output for every entry in validation/test dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    return [Majority(computeDistancesNeighbors(K, metric_type, X_train, y_train, sample)) for sample in X_val]\n",
    "\n",
    "\n",
    "def evaluation(predicted_values, actual_values):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the given datapoints.\n",
    "    \n",
    "    Args:\n",
    "        predicted_values ((n,) np.ndarray): Predicted values for n samples\n",
    "        actual_values ((n,) np.ndarray): Actual values for n samples\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    return accuracy_score(predicted_values, actual_values)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['K', 'norm', 'val-accuracy', 'test-accuracy'])\n",
    "\n",
    "def main():\n",
    "    global df\n",
    "    \"\"\"\n",
    "    Calls the above functions in order to implement the KNN algorithm.\n",
    "    \n",
    "    Test over the following range K = 3,5,7 and all three metrics. \n",
    "    In total you will have nine combinations to try.\n",
    "    \n",
    "    PRINTS out the accuracies for the nine combinations on the validation set,\n",
    "    and the accuracy on the test set for the selected K value and appropriate norm.\n",
    "    \n",
    "    REMEMBER: You have to report these values by populating the Table 1.\n",
    "    \"\"\"\n",
    "\n",
    "    K = [3,5,7]\n",
    "    norm = [\"L1\", \"L2\", \"L-inf\"]\n",
    "    \n",
    "    print(\"<<<<VALIDATION DATA PREDICTIONS>>>>\")\n",
    "    for k, n in product(K, norm):\n",
    "        acc = evaluation(KNN(k, n, X_train, y_train, X_val), y_val)\n",
    "        print(f'K = {k}, Norm = {n}, Accuracy = {acc}')\n",
    "        df = df.append({'K': k, 'norm': n, 'val-accuracy': acc}, ignore_index=True)\n",
    "\n",
    "    print(\"<<<<TEST DATA PREDICTIONS>>>>\")\n",
    "    for k, n in product(K, norm):\n",
    "        acc = evaluation(KNN(k, n, X_train, y_train, X_test), y_test)\n",
    "        print(f\"K = {k}, Norm = {n}, Accuracy = {acc}\")\n",
    "        df.loc[(df['K'] == k) & (df['norm'] == n), 'test-accuracy'] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code below to run the main function (Remember to recomment the code before submitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, call the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions here:\n",
    "\n",
    "1. How could having a larger dataset influence the performance of KNN?\n",
    "\n",
    "- KNN actually does nothing in the training phase (it just collect the data), but when prediction, it must iterate through the whole dataset to find the nearest neighbors, which would increase the predicting time, aka. $O(pred) = O(n)$ w.r.t. $n$ is the size of the dataset.\n",
    "\n",
    "2. Tabulate your results from `main()` in the table provided.\n",
    "\n",
    "|    |   K | norm   |   val-accuracy |   test-accuracy |\n",
    "|---:|----:|:-------|---------------:|----------------:|\n",
    "|  0 |   3 | L1     |           0.94 |        0.884058 |\n",
    "|  1 |   3 | L2     |           0.95 |        0.884058 |\n",
    "|  2 |   3 | L-inf  |           0.94 |        0.898551 |\n",
    "|  3 |   5 | L1     |           0.94 |        0.913043 |\n",
    "|  4 |   5 | L2     |           0.93 |        0.898551 |\n",
    "|  5 |   5 | L-inf  |           0.94 |        0.898551 |\n",
    "|  6 |   7 | L1     |           0.93 |        0.898551 |\n",
    "|  7 |   7 | L2     |           0.92 |        0.913043 |\n",
    "|  8 |   7 | L-inf  |           0.93 |        0.898551 |\n",
    "\n",
    "3. Finally, mention the best K and the norm combination you have settled upon and report the accuracy on the test set using that combination.\n",
    "\n",
    "Setting `K` as 5 with `L1` metric, or setting `K` as 7 with `L2` metric, both get the highest score. However, the first combination may have a better (speed) performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sci39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d41f7ce290d37fe5b4ab4e5bf6b63bd16e05157bfb17b66f4f27316420edd9a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
