1. Can SVM be used for unsupervised clustering or data dimension reduction? Why?

    Yes, Stephen and Sam (2007, doi: 10.1186/1471-2105-8-S7-S18) experimented on an algorithm that improves on its weakly convergent result by SVM re-training after each re-labeling on the worst of the misclassified vectors, i.e. those feature vectors with confidence factor values beyond some threshold. The repetition of the above process improves the accuracy, here a measure of separability, until there are no misclassifications, showing the ability of SVM to be used for unsupervised clustering. Similarly, SVM can be used for data dimension reduction.


2. What are the strengths of SVMs; when do they perform well?
    - When there is a clear margin of separation between classes.
    - When the data is in high dimensions.
    - It's effective when the number of dimensions is greater than the number of samples.
    - It saves memory.

3. What are the weaknesses of SVMs; when do they perform poorly?
    - It's not suitable for large data sets.
    - It performs poor when the data comes with much noise.
    - When the number of features for each data point exceeds the number of training data samples, SVM underperforms.

4. What makes SVMs a good candidate for the classification / regression problem, if you have enough knowledge about the data?
    - The data is in high dimensions, with a clear margin of separation between classes.
