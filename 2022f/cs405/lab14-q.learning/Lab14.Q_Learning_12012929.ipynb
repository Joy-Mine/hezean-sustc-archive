{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB Assignment\n",
    "Please finish the **Exercise** and answer **Questions**.\n",
    "## Exercise ( Q-Learning with Taxi-v3 üöï) (100 Points)\n",
    "\n",
    "In this exercise, you should complete the Q-learning algorithm using the Taxi-v3 environment in the gym\n",
    "\n",
    "In Taxi-v3 üöï, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger‚Äôs location, picks up the passenger, drives to the passenger‚Äôs destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "<div align=\"center\"><img src=\"images/image-20220805133926061.png\" alt=\"image-20220805133926061\" style=\"zoom:80%;\" /></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import imageio\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 1 Create Taxi-v3 üöï  environment \n",
    "Using the API imported from gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger** (including the case when the passenger is in the taxi), and **4 destination locations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space (the set of possible actions the agent can take) is discrete with **6 actions available üéÆ**:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "Reward function üí∞:\n",
    "- -1 per step unless other reward is triggered.\n",
    "- +20 delivering passenger.\n",
    "- -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 2  Create the Q-table and initialize it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the gym api to fetch the dimension of action space and state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.n\n",
    "\n",
    "# Please complete this initialization in this line\n",
    "Q_table = np.zeros((state_space, action_space))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 3 Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 100000        # ‰∏ÄÂÖ±Áé©Â§öÂ∞ëÂ±ÄÊ∏∏Êàè\n",
    "total_test_episodes = 100     # ÊµãËØï‰∏≠‰∏ÄÂÖ±Ëµ∞Âá†Ê≠•\n",
    "max_steps = 99                # Max steps per episode ÊØè‰∏ÄÂ±ÄÊ∏∏ÊàèÊúÄÂ§öËµ∞Âá†Ê≠•\n",
    "\n",
    "learning_rate = 0.5           # Learning rate\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05           # Minimum exploration probability \n",
    "decay_rate = 0.008            # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "test_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
    " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
    " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4 Q Learning algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The formula of Q table update(Bellman equation)\n",
    "    ![Bellman equation](https://raw.githubusercontent.com/hanruihua/NoteBook/master/AI-Note/equation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = tqdm.tqdm(total=total_episodes)\n",
    "sample_rewards = []\n",
    "for episode in range(total_episodes):\n",
    "    state= env.reset()\n",
    "    step=0\n",
    "    done=False\n",
    "    sample_reward = 0 \n",
    "    while True:\n",
    "        action = np.argmax(Q_table[state, :])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # Calculate the reward of this episode\n",
    "        sample_reward += reward\n",
    "        \n",
    "        # Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        Q_table[state, action] = Q_table[state, action] + learning_rate * (reward + gamma * np.max(Q_table[new_state, :]) - Q_table[state, action])\n",
    "        \n",
    "        # Update the state\n",
    "        state = new_state\n",
    "        \n",
    "        #store the episode reward\n",
    "        if done == True:\n",
    "            sample_rewards.append(sample_reward)\n",
    "            break\n",
    "    # Reduced exploration probability (due to decreasing uncertainty)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)         \n",
    "    # print the average reward over 1000 episodes\n",
    "    if episode%1000 == 0:\n",
    "        mean_reward = np.mean(sample_rewards)\n",
    "        sample_rewards = []\n",
    "        #print(str(episode)+\": average reward:\" + str(mean_reward))\n",
    "        bar.set_description(str(episode)+\": average reward:\" + str(mean_reward))\n",
    "    bar.update()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps=5\n",
    "bar = tqdm.tqdm(total=total_test_episodes)\n",
    "env.reset()\n",
    "rewards=[]\n",
    "images = [] \n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset(seed=test_seed[episode])\n",
    "    step = 0\n",
    "    done =False\n",
    "    total_rewards = 0\n",
    "   \n",
    "    for step in range(max_steps):\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "        action = np.argmax(Q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "     \n",
    "env.close()\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "imageio.mimsave('taxi-v3.gif', [np.array(img) for i, img in enumerate(images)], fps=fps)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 6 Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('taxi-v3.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sci39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d41f7ce290d37fe5b4ab4e5bf6b63bd16e05157bfb17b66f4f27316420edd9a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
